{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2edec546",
   "metadata": {},
   "source": [
    "### Homework: Search Evaluation\n",
    "\n",
    "In this homework, we will evaluate the results of vector search.\n",
    "\n",
    "It's possible that your answers will not match exactly. If that is the case, select the closest one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6313a857",
   "metadata": {},
   "source": [
    "### Required Libraries\n",
    "\n",
    "We will use minsearch and Qdrant. Make sure you have the most up-to-date versions:\n",
    "\n",
    "`pip install -U minsearch qdrant_client rouge`\n",
    "\n",
    "minsearch should be at least 0.0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f02a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U minsearch qdrant_client rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b0fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import minsearch\n",
    "from minsearch import VectorSearch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from rouge import Rouge\n",
    "from fastembed import TextEmbedding\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct, CollectionStatus, CollectionInfo\n",
    "from qdrant_client import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aba693b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.4\n"
     ]
    }
   ],
   "source": [
    "print(minsearch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdfe6f7",
   "metadata": {},
   "source": [
    "### Evaluation Data\n",
    "\n",
    "For this homework, we will use the same dataset we generated in the videos.\n",
    "\n",
    "Let's get them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e30b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_prefix = 'https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/'\n",
    "docs_url = url_prefix + 'search_evaluation/documents-with-ids.json'\n",
    "documents = requests.get(docs_url).json()\n",
    "\n",
    "ground_truth_url = url_prefix + 'search_evaluation/ground-truth-data.csv'\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b872e83",
   "metadata": {},
   "source": [
    "Here, `documents` contains the documents from the FAQ database with unique IDs, and `ground_truth` contains generated question-answer pairs.\n",
    "\n",
    "Also, we will need the code for evaluating retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04715165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['document']\n",
    "        results = search_function(q)\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04af9f5",
   "metadata": {},
   "source": [
    "### Q1. Minsearch Text\n",
    "\n",
    "Now let's evaluate our usual minsearch approach, indexing documents with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bdabe66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x738a52bc1820>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\", \"id\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4673d3dd",
   "metadata": {},
   "source": [
    "but tweak the parameters for search. Let's use the following boosting parameters:\n",
    "\n",
    "`boost = {'question': 1.5, 'section': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e534905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_search(query, course):\n",
    "    boost = {'question': 1.5, 'section': 0.1}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': course},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0743b051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df22972cf7944764961835d292b9127d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relevance_total = []\n",
    "\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    results = minsearch_search(query=q['question'], course=q['course'])\n",
    "    relevance = [d['id'] == doc_id for d in results]\n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9472bd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.848714069591528, 0.7288235717887772)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_rate(relevance_total), mrr(relevance_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607027e1",
   "metadata": {},
   "source": [
    "What's the hitrate for this approach?\n",
    "\n",
    "- 0.64\n",
    "- 0.74\n",
    "- 0.84\n",
    "- 0.94"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5dc794",
   "metadata": {},
   "source": [
    "### A1. The Hit Rate using minsearch and boosting the question by 1.5 and the section by 0.1 is 0.84."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452d0afc",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "The latest version of minsearch also supports vector search.  We will use it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f697a4",
   "metadata": {},
   "source": [
    "We will also use TF-IDF and Singular Value Decomposition to \n",
    "create embeddings from texts. You can refer to our\n",
    "[\"Create Your Own Search Engine\" workshop](https://github.com/alexeygrigorev/build-your-own-search-engine)\n",
    "if you want to know more about it.\n",
    "\n",
    "Let's create embeddings for the \"question\" field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "609f1932",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "\n",
    "for doc in documents:\n",
    "    t = doc['question']\n",
    "    texts.append(t)\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "X = pipeline.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b87a53f",
   "metadata": {},
   "source": [
    "### Q2. Vector Search for Question\n",
    "\n",
    "Now let's index these embeddings with minsearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0d4517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x738a51d37b00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vindex = VectorSearch(keyword_fields={'course'})\n",
    "vindex.fit(X, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c87e1488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf5fc0fb731420db827bd619f4b2953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relevance_total = []\n",
    "\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    query_vec = pipeline.transform([q['question']])[0]\n",
    "    results = vindex.search(query_vec, filter_dict={'course': q['course']}, num_results=5)\n",
    "    relevance = [d['id'] == doc_id for d in results]\n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0009cc3",
   "metadata": {},
   "source": [
    "Evaluate this search method. What is the MRR for it?\n",
    "\n",
    "- 0.25\n",
    "- 0.35\n",
    "- 0.45\n",
    "- 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebb99c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3572833369353793"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr(relevance_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe570376",
   "metadata": {},
   "source": [
    "### A2. The MRR is 0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41319863",
   "metadata": {},
   "source": [
    "### Q3. Vector Search for Question and Answer\n",
    "\n",
    "We only used question in Q2. We can use both question and anser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9a26041",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "\n",
    "for doc in documents:\n",
    "    t = doc['question'] + ' ' + doc['text']\n",
    "    texts.append(t) \n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "X = pipeline.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e1ccc",
   "metadata": {},
   "source": [
    "Using the same pipeline (`min_df=3` for TF-IDF vectorizer and `n_components=128` for SVD), evaluate the performance of this approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd083ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x738a51367590>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vindex = VectorSearch(keyword_fields={'course'})\n",
    "vindex.fit(X, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52ddba65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['question', 'course', 'document'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_ground_truth.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10d46f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036614516fd846939cd61e8cce4ca75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relevance_total = []\n",
    "\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    query_text = q['question']\n",
    "    query_vec = pipeline.transform([query_text])[0]\n",
    "    results = vindex.search(query_vec, filter_dict={'course': q['course']}, num_results=5)\n",
    "    relevance = [d['id'] == doc_id for d in results]\n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e81e09",
   "metadata": {},
   "source": [
    "What is the hitrate?\n",
    "\n",
    "- 0.62\n",
    "- 0.72\n",
    "- 0.82\n",
    "- 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83828414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8210503566025502, 0.6717347453353508)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_rate(relevance_total), mrr(relevance_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c48d2e",
   "metadata": {},
   "source": [
    "### A3. The Hitrate is 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229d24da",
   "metadata": {},
   "source": [
    "### Q4. Qdrant\n",
    "\n",
    "Now let's evaluate the following settings in Qdrant:\n",
    "\n",
    "- `text = doc['question'] + ' ' + doc['text']`\n",
    "- `model_handle = \"jinaai/jina-embeddings-v2-small-en\"`\n",
    "- `limit = 5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06c4282",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\":memory:\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebbdaa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "EMBEDDING_DIMENSIONALITY = 512\n",
    "embedder = TextEmbedding(model_name=model_handle)\n",
    "\n",
    "# Prepare the texts for embedding\n",
    "texts = [doc['question'] + ' ' + doc['text'] for doc in documents]\n",
    "\n",
    "# Use batching to avoid memory issues\n",
    "batch_size = 8\n",
    "embeddings = []\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    batch_embeddings = list(embedder.embed(batch))\n",
    "    embeddings.extend(batch_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5652f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"search-evaluation\"\n",
    "    \n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=EMBEDDING_DIMENSIONALITY,  # Dimensionality of the vectors\n",
    "        distance=models.Distance.COSINE  # Distance metric for similarity search\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model.encode([doc['question'] + ' ' + doc['text']])[0]\n",
    "points = []\n",
    "for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "    point = PointStruct(\n",
    "        id=i, # PointStruct requires a unique ID, using integer index here\n",
    "        vector=embedding.tolist(),  # Convert numpy array to list\n",
    "        payload={\n",
    "            \"question\": doc['question'],\n",
    "            \"text\": doc['text'],\n",
    "            \"course\": doc['course'],\n",
    "            \"section\": doc['section'],\n",
    "            \"id\": doc['id']  # Keep original document ID in payload\n",
    "        }\n",
    "    )\n",
    "    points.append(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2fc827",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_total = []\n",
    "\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    query_text = q['question'] + ' ' + q['answer_orig']\n",
    "    query_embedding = embed(query_text) \n",
    "\n",
    "    search_result = qdrant_client.search(\n",
    "        collection_name=\"your_collection\",\n",
    "        query_vector=query_embedding,\n",
    "        limit=5,\n",
    "        filter={\"must\": [{\"key\": \"course\", \"match\": {\"value\": q[\"course\"]}}]}\n",
    "    )\n",
    "\n",
    "    relevance = [point.payload['id'] == doc_id for point in search_result]\n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a64875",
   "metadata": {},
   "source": [
    "What's the MRR?\n",
    "\n",
    "- 0.65\n",
    "- 0.75\n",
    "- 0.85\n",
    "- 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d767be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr(relevance_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62be2cd",
   "metadata": {},
   "source": [
    "### A4. The MRR is "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b231496",
   "metadata": {},
   "source": [
    "### Q5. Cosine Similarity\n",
    "\n",
    "In the second part of the module, we looked at evaluating the entire RAG approach.  In particular, we looked at comparing the answer generated by our system with the actual answer from the FAQ.\n",
    "\n",
    "One of the ways of doing it is by using the cosine similarity. Let's see how to calculate it.\n",
    "\n",
    "Cosine similarity is a dot product between two normalized vectors. In a geometrical sense, it's the cosine of the angles between the vectors.  Look up \"cosine similarity geometry\" if you want to learn more about it.\n",
    "\n",
    "For us, it means that we need two things:\n",
    "- First, we normalize each of the vectors\n",
    "- Then, we compute the dot product\n",
    "\n",
    "So we get this:\n",
    "\n",
    "```python\n",
    "def cosine(u, v):\n",
    "    u = normalize(u)\n",
    "    v = normalize(v)\n",
    "    return u.dot(v)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b199b",
   "metadata": {},
   "source": [
    "For normalization, we first conmpute the vector norm (its length), and then divide the vector by it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c1fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(u):\n",
    "    norm = np.sqrt(u.dot(u))\n",
    "    return u / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d09e523",
   "metadata": {},
   "source": [
    "Or we can simplify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db24bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    u_norm = np.sqrt(u.dot(u))\n",
    "    v_norm = np.sqrt(v.dot(v))\n",
    "    return u.dot(v) / (u_norm * v_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e14589b",
   "metadata": {},
   "source": [
    "Now let's use this function to compute the A->Q->A cosine similarity.\n",
    "\n",
    "We will use the results from [our gpt-4o-mini evaluations](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/03-evaluation/rag_evaluation/data/results-gpt4o-mini.csv):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfe7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_url = url_prefix + 'rag_evaluation/data/results-gpt4o-mini.csv'\n",
    "df_results = pd.read_csv(results_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d6bf92",
   "metadata": {},
   "source": [
    "When creating the embeddings, we will use a simple way - the same we used in the [Embeddings](#embeddings) section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5be27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843e994f",
   "metadata": {},
   "source": [
    "Let's fit the vectorizer on all the text data we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(df_results.answer_llm + ' ' + df_results.answer_orig + ' ' + df_results.question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c64587",
   "metadata": {},
   "source": [
    "Now use the `transform` method of the pipeline to create the embeddings and calculate the cosine similarity between each pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1666be9",
   "metadata": {},
   "source": [
    "This is how you do it:\n",
    "\n",
    "- For each answer pair, compute\n",
    "   - `v_llm` for the answer from the LLM\n",
    "   - `v_orig` for the original answer\n",
    "   - then compute the cosine between them\n",
    "- At the end, take the average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6448d53",
   "metadata": {},
   "source": [
    "### Q6. Rouge\n",
    "\n",
    "An alternative way to see how two texts are similar is ROUGE.\n",
    "\n",
    "This is a set of metrics that compares two answers based on the overlap of n-grams, word sequences, and word pairs.\n",
    "\n",
    "It can give a more nuanced view of text similarity than just cosine similarity alone.\n",
    "\n",
    "We don't need to implement it ourselves, there's a python package for it: `pip install rouge`\n",
    "\n",
    "(The latest version at the moment of writing is 1.0.1)\n",
    "\n",
    "Let's compute the ROUGE score between the answers at the index 10 of our dataframe (`doc_id=5170565b`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7cf79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scorer = Rouge()\n",
    "\n",
    "r = df_results.iloc[10]\n",
    "scores = rouge_scorer.get_scores(r.answer_llm, r.answer_orig)[0]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab21e8c",
   "metadata": {},
   "source": [
    "There are three scores: `rouge-1`, `rouge-2`, and `rouge-l`, and precision, recall and F1 scores for each.\n",
    "\n",
    "- `rouge-1` - the overlap of unigrams\n",
    "- `rouge-2` - bigrams\n",
    "- `rouge-l` - the longest common subsequence\n",
    "\n",
    "For the 10th document, Rouge-1 F1 score is 0.45\n",
    "\n",
    "Let's compute it for the pairs in the entire dataframe.  What's the average Rouge-1 F1?\n",
    "\n",
    "- 0.25\n",
    "- 0.35\n",
    "- 0.45\n",
    "- 0.55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c939d1b",
   "metadata": {},
   "source": [
    "### A6. The average Rouge-1 F1 is"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
